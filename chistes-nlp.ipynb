{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP applied to classify Spanish jokes\n",
    "\n",
    "In this notebook we will be applying different Natural Language Processing techniques to a corpus of jokes in Spanish.\n",
    "The **objective** is to **train a Machine Learning model to classify jokes in categories**.\n",
    "\n",
    "In order to execute smoothly the code, you should've installed the requirements using `pipenv` or `pip` (refer to the README.md for details).\n",
    "\n",
    "Actually we will be using **pandas** to do a first exploration of the dataset, **spacy (with Spanish package installed)** to extract Natural Language information from the jokes, and **sklearn** to vectorize the jokes and train a Machine Learning model to classify jokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter config\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data science stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chistes.csv', index_col='id', dtype={'text':str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be focusing on joke's text and category. Let's see how many jokes do we have and the different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have ~2400 jokes with 4 columns defining jokes' details. Let's explore how many categories are in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 7 categories, being the most common \"otros\" with a 31.8%.\n",
    "\n",
    "Let's explore the size of the jokes too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'] = df['text'].apply(lambda t: len(t))\n",
    "\n",
    "df['len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['len'].hist(bins=200, figsize=(16, 4))\n",
    "\n",
    "plt.xticks(range(0,2000,50))\n",
    "plt.xlim((0,1000))\n",
    "plt.axvline(df['len'].median(), color='r')  # Median in red\n",
    "plt.axvline(df['len'].mean(), color='g')  # Mean in green\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half of the jokes have a size below 160 characters, which looks like really short documents to try NLP.\n",
    "\n",
    "\n",
    "## Using spacy info\n",
    "\n",
    "We will load Spanish module in spacy and try to get Part Of Speech (POS) of each word and other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"es\")\n",
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get a random joke and process it with spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_joke = df.iloc[10]['text']\n",
    "\n",
    "print(random_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_joke = nlp(random_joke)\n",
    "\n",
    "for token in processed_joke:\n",
    "    print(token.text,'\\t lemma:', token.lemma_, ', pos:', token.pos_, ', tag:', token.tag_, ', stopword:', token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy provides us with some useful information. In this case:\n",
    "* lemma: the *dictionary form* of the word\n",
    "* pos: part of speech, for example: noun, verb, adjective...\n",
    "* tag: part of speech with extended info, like gender, number, etc\n",
    "* stopword: if the word is considered meaningless (for NLP tasks) or not\n",
    "\n",
    "As you see, spacy sometimes gives us wrong info:\n",
    "* the lemma of \"pelo\" should be \"pelo\"(noun) and not \"pelar\"(verb)\n",
    "* \"durmiente\" should've been tagged as noun, but it's tagged as adverb\n",
    "\n",
    "Let's stress spacy with a *classic* complex sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complex_sentence = 'Bajo con un tipo bajo a tocar el bajo bajo la escalera.'\n",
    "processed = nlp(complex_sentence)\n",
    "for token in processed:\n",
    "    print(token.text, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first \"bajo\" should be a verb, but it was tagged as preposition. However the other 3 \"bajo\" are correctly labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn for vectorization\n",
    "\n",
    "An easy way to convert a document into numbers (so algorithms can be easily applied to) is to count the words it contains. Usually it's better to consider only words that have strong meaning, and in our case (try to classify documents) it's important to find words that are common enough, to convert them into \"features\", but not too common, so they can help us classifying the documents.\n",
    "\n",
    "In sklearn there are several methods to count words from documents. In the next example, we will be looking for the 20 most common words, but that appear in less than 200 jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "max_jokes_with_that_word = 200\n",
    "\n",
    "vectorizer20 = CountVectorizer(max_df=max_jokes_with_that_word, max_features=20)\n",
    "bag_of_words = vectorizer20.fit_transform(df.text)\n",
    "\n",
    "vectorizer20.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most relevant words are a mix of good ones (like \"doctor\") and not so good (like \"soy\"). The result is a *bag of words*, that is, a matrix with the counts of each relevant words in each joke.\n",
    "\n",
    "The bag of words here is a sparse matrix, but we can convert it to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = pd.DataFrame(data=bag_of_words.toarray(), index=df.index, columns=vectorizer20.get_feature_names())\n",
    "counted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, the word \"mamá\" appears 2 times in the joke with id=7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering words\n",
    "\n",
    "Let's try to keep only words with real meaning. A classic way to do so is removing all the stopwords, but here we can leverage spacy extra information and keep only nouns, verbs, adjectives and adverbs.\n",
    "\n",
    "In the old times they used to process the words with a stemmer (that removes the ending \"s\" and some other tricks), but we are going to normalize the words using their lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_content_words(s):\n",
    "    processed = nlp(s)\n",
    "    result = [token.lemma_ for token in processed if token.pos_ in ('NOUN', 'VERB', 'ADJ', 'ADV')]\n",
    "    return ' '.join(result)\n",
    "\n",
    "print(random_joke)\n",
    "print()\n",
    "print(keep_only_content_words(random_joke))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filtered_text'] = df['text'].apply(keep_only_content_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try again to find the most common 20 words, only using the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer20 = CountVectorizer(max_df=max_jokes_with_that_word, max_features=20)\n",
    "bag_of_words = vectorizer20.fit_transform(df.filtered_text)\n",
    "\n",
    "vectorizer20.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see it in a table\n",
    "counted = pd.DataFrame(data=bag_of_words.toarray(), index=df.index, columns=vectorizer20.get_feature_names())\n",
    "counted.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf\n",
    "\n",
    "A better way to make numerical values of the words is using tf-idf (term frequency, inverse document frequency). Let's apply it to our bag_of_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_trans = TfidfTransformer()\n",
    "\n",
    "normalized_bag = tfidf_trans.fit_transform(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display word importance\n",
    "pd.DataFrame(data=normalized_bag.toarray(), index=df.index, columns=vectorizer20.get_feature_names()).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real vectorizer with tf-idf\n",
    "\n",
    "Finally we are going to find no just 20 but the most relevant 500 words, and use these later for ML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer = CountVectorizer + TFidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=max_jokes_with_that_word, max_features=500)\n",
    "bag_of_words = tfidf_vectorizer.fit_transform(df.filtered_text)\n",
    "\n",
    "important_words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print(', '.join(important_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a ML algorithm\n",
    "\n",
    "Now we have 500 features per document (joke). We are going to train a ML algorithm to learn the 8 categories provided.\n",
    "\n",
    "Usually the collection of samples (documents, jokes) with their features is called \"X\", and the target is called \"y\" (in our case, the categories).\n",
    "\n",
    "First we will convert the categories to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y : let's make category a number\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n",
    "y = enc.fit_transform(df.category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = y\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split X into a train set and a test set. So we will be training the ML algorithm ONLY with the train set, and later see how well it preforms with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split products in train (75%) and test (25%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = bag_of_words\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a training set with 1814 jokes and 500 features per joke, and a test set of 605 jokes.\n",
    "\n",
    "### Train a Random Forest\n",
    "\n",
    "We are going to train a RandomForestClassifier with 200 trees, and see if we can beat the base score (that is, supose all jokes are in \"otros\" category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "number_of_trees = 200\n",
    "clf = RandomForestClassifier(n_estimators=number_of_trees, random_state=1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "base_score = 0.318\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Base score (all to \"otros\"): {base_score}')\n",
    "print(f'Train set score: {train_score}')\n",
    "print(f'Test set score: {test_score}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! We have improved 16 points the score. However the result is far from perfect, probably due to the short size of the jokes.\n",
    "\n",
    "Let's check which features(words) were the most relevant for deciding the category of a joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = pd.DataFrame(data=clf.feature_importances_, index=tfidf_vectorizer.get_feature_names(), columns=['importance'])\n",
    "importances.sort_values(['importance'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remember categories\n",
    "df['category'].value_counts()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the most important words make sense, like perro->animales. Other like \"maridar\"?! or \"llamar\" are not that clear.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**EXERCISE** : Try to improve the result of the ML algorithm.\n",
    "\n",
    "You can check 3 alternatives:\n",
    "- In the section \"Filtering words\", choose using the lemma or not, and different POS.\n",
    "- In the section \"Real vectorizer with tf-idf\", consider more than 500 words.\n",
    "- In the section \"Train a Random Forest\", explore RandomForestClassifier options (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).\n",
    "\n",
    "Notice: you can NOT change the train/test splitting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Having a second look at the categories provided, it looks like they are not really good (specially if we look at \"otros\").\n",
    "\n",
    "There are several unsupervised techniques to, given a collection of documents, find groups of topics. \n",
    "\n",
    "We will try here LaternDirichletAllocation, or LDA, which is a classic tech but usually hard to work with, as it needs a lot of fine tunning. For instance, choosing the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "\n",
    "lda.fit(X)\n",
    "topics = lda.transform(X)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "            \n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "print_top_words(lda, tfidf_vectorizer.get_feature_names(), 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the topics make sense, like topic #5 (mother and school) or topic #8 (father), but others show no clear understandable topic.\n",
    "\n",
    "Let's try to visualize the weights for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_with_topics_weights = pd.concat([df, pd.DataFrame(topics, columns=[f'topic_{x}' for x in range(0,10)])], axis=1)\n",
    "jokes_with_topics_weights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find some strong examples of topic #1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic1 = jokes_with_topics_weights[jokes_with_topics_weights['topic_1']>0.80]\n",
    "topic1['text'].apply(lambda s: print(s+'\\n-----\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently there is not a clear subject in common.\n",
    "\n",
    "**EXERCISE** : Try to explore other topics, change the number of topics, or alter [LDA hyperparams](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "Spacy includes (in each language pack) a vectors for words from word2vec. We can access easily using .vector property in a token, but also in a complete text, as it returns the average of each word in this case. You can also use .similarity(other_words) to check cosine similarity between two words.\n",
    "\n",
    "Let's try some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocidad = nlp('velocidad')\n",
    "velocidad.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aceleracion = nlp('aceleración')\n",
    "tocino = nlp('tocino')\n",
    "\n",
    "print(velocidad.similarity(aceleracion))\n",
    "print(velocidad.similarity(tocino))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Process all jokes to get their vectors\n",
    "\n",
    "Let's calculate the vectors for all jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes = df['text']\n",
    "vectors = []\n",
    "\n",
    "for index,joke in jokes.iteritems():\n",
    "    vectors.append(nlp(joke).vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the vectors to estimate categories using ML\n",
    "\n",
    "We are going to split again train and test set, and use RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv = vectors\n",
    "\n",
    "Xv_train, Xv_test, y_train, y_test = train_test_split(Xv, y, random_state=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_trees = 200\n",
    "clf = RandomForestClassifier(n_estimators=number_of_trees, random_state=1)\n",
    "clf.fit(Xv_train, y_train)\n",
    "\n",
    "base_score = 0.318\n",
    "train_score = clf.score(Xv_train, y_train)\n",
    "test_score = clf.score(Xv_test, y_test)\n",
    "\n",
    "print(f'Base score (all to \"otros\"): {base_score}')\n",
    "print(f'Train set score: {train_score}')\n",
    "print(f'Test set score: {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is not really surprising. Perhaps because the original categories were not good enough.\n",
    "\n",
    "**EXERCISE** : Try to, for each joke, get only the average vector of the filtered words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Finally, just to see how were the jokes classified, we can reduce the dimensionality and plot it in 2D. Knowing the categories, we can use Linear Discriminant Analysis (if not know, PCA is more common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "\n",
    "X_reduced = lda.fit_transform(X.toarray(), y)\n",
    "\n",
    "X_reduced = pd.DataFrame(X_reduced, columns={'x', 'y'})\n",
    "X_reduced['color'] = df['y'].apply(lambda cat: 'rgbcmyk'[cat])\n",
    "X_reduced.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.scatter(X_reduced['x'], X_reduced['y'], color=X_reduced['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that only red is out of the mass, which is category 0: animales.\n",
    "Green (category 1: familia) and yellow (cat 5: sexo) are similar.\n",
    "And the rest is a mess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for joining this workshop!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
